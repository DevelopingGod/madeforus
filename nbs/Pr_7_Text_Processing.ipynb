{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Name: Sankalp Indish\n",
        "## Class: TE-B\n",
        "## Batch: B4\n",
        "## Subject: DSBDAL\n",
        "## Roll No: TEB75\n",
        "## Practical 7: Text Analytics]\n",
        "- 1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
        "- 2. Create representation of document by calculating Term Frequency and Inverse Document Frequency.\n"
      ],
      "metadata": {
        "id": "yN8LAEm1ORYx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DcKq34AOCn1",
        "outputId": "5f2112c9-e155-4ba7-90ab-2ad5817ce208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the other required libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import string"
      ],
      "metadata": {
        "id": "lxaQ3lkhOjrM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK datasets [ for processing]\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRfnNGV8Oomy",
        "outputId": "d2829071-e7bd-400b-b977-53c14314713d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gDW-85WOxfE",
        "outputId": "6d4b00f0-0f7e-4b6e-91b3-0eb0f7ca7832"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHvEVKLkO1RO",
        "outputId": "585f5c83-075a-456c-d687-666e781700aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "text = \"Machine learning is very very important because it allows computers to learn from data.\""
      ],
      "metadata": {
        "id": "ES-E4QLnOtCq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Tokenization"
      ],
      "metadata": {
        "id": "SZTHr9k0PAJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(text)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSRDMLiaOvkB",
        "outputId": "52a629a8-09a2-4480-9d39-e19044ef0f16"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Machine',\n",
              " 'learning',\n",
              " 'is',\n",
              " 'very',\n",
              " 'very',\n",
              " 'important',\n",
              " 'because',\n",
              " 'it',\n",
              " 'allows',\n",
              " 'computers',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'from',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: POS Tagging"
      ],
      "metadata": {
        "id": "Htk6dNi7PINJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = pos_tag(tokens)\n",
        "pos_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPIZ3TaWPEf5",
        "outputId": "b82fc5f5-5992-4a14-8046-5caddd728587"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Machine', 'NN'),\n",
              " ('learning', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('very', 'RB'),\n",
              " ('very', 'RB'),\n",
              " ('important', 'JJ'),\n",
              " ('because', 'IN'),\n",
              " ('it', 'PRP'),\n",
              " ('allows', 'VBZ'),\n",
              " ('computers', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('learn', 'VB'),\n",
              " ('from', 'IN'),\n",
              " ('data', 'NNS'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Stop words Removal"
      ],
      "metadata": {
        "id": "GZ4tXtprPVBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae9yeJQ5PMlK",
        "outputId": "63ec990f-6366-464e-a71b-8b1c8e1c645b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"it'll\", 'only', \"needn't\", 'they', 'd', 'most', 'shouldn', 'again', \"he'd\", 'won', 'are', \"i'm\", 'you', 'more', 'does', \"shouldn't\", \"you've\", 'below', \"she's\", 'was', 'some', 'just', 'so', 'other', 'themselves', \"we're\", 'over', \"wouldn't\", 'hadn', \"won't\", 'through', \"they'd\", 'me', 'ours', 'and', 'in', \"i'll\", \"hasn't\", 'the', 'this', 'how', 'i', 'mustn', \"they're\", 'few', 'theirs', 'its', 'on', \"she'll\", 'isn', \"they'll\", 'haven', 'y', 'because', 'been', 'your', 'off', 'his', 'for', 'why', \"he'll\", 'their', \"we'll\", \"you're\", 'who', 'doesn', \"that'll\", 'between', \"shan't\", 'these', 'having', \"didn't\", 'both', 'them', 'couldn', 'out', 'with', 'him', \"we've\", 'not', 'or', 'no', \"they've\", 'didn', \"isn't\", 'nor', 'against', 'which', 'where', 'further', 'yourself', 'own', \"weren't\", 'will', \"it's\", 't', \"don't\", 'after', 'what', \"haven't\", \"aren't\", 'an', 'it', 'than', 'had', 'wouldn', \"mustn't\", \"hadn't\", 'while', 'during', 'is', 'were', 'ourselves', 'too', 'aren', 'did', 'o', 'am', 'll', \"i'd\", 'here', 'very', 'about', 'hasn', \"mightn't\", 'm', 'myself', 're', \"you'll\", \"it'd\", \"you'd\", 'hers', 'a', \"wasn't\", 'itself', \"should've\", 'yours', \"doesn't\", 'ain', 'he', 'herself', 'same', \"we'd\", 's', 'any', 'before', 'down', 've', 'we', \"i've\", 'such', 'wasn', 'yourselves', 'from', 'has', 'if', 'but', 'don', 'under', 'ma', 'being', \"he's\", 'should', 'do', 'as', 'once', 'there', 'by', 'all', 'at', 'to', 'can', 'when', 'of', 'shan', \"couldn't\", 'doing', 'mightn', 'needn', 'himself', 'she', 'until', 'up', 'be', 'whom', 'my', 'her', 'that', \"she'd\", 'into', 'above', 'then', 'each', 'have', 'our', 'weren', 'now', 'those'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now filtering our tokens\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n",
        "filtered_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31ScvmN3PaNR",
        "outputId": "8889edc9-5517-439b-9188-e48c5c1c7396"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Machine', 'learning', 'important', 'allows', 'computers', 'learn', 'data']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Stemming"
      ],
      "metadata": {
        "id": "xTYon_12QEIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "stemmed_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRbu2GakQAlB",
        "outputId": "9192d877-b41d-47a5-884d-706d94ae5bc4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['machin', 'learn', 'import', 'allow', 'comput', 'learn', 'data']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Lemmatization"
      ],
      "metadata": {
        "id": "d2-3-om7QKOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "lemmatized_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcjYgcMBQIbx",
        "outputId": "112d6ba9-2f38-4603-e173-cd251a56b30c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Machine', 'learning', 'important', 'allows', 'computer', 'learn', 'data']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Evaluation and Metric Calculation"
      ],
      "metadata": {
        "id": "DK7tRG0AQSEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Term Frequency (TF)\n",
        "word_count = len(filtered_tokens)\n",
        "tf = {word: filtered_tokens.count(word) / word_count for word in filtered_tokens}\n",
        "tf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTSeoF5FQNyp",
        "outputId": "6d243511-b61f-4236-e2ba-b188817ea85b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Machine': 0.14285714285714285,\n",
              " 'learning': 0.14285714285714285,\n",
              " 'important': 0.14285714285714285,\n",
              " 'allows': 0.14285714285714285,\n",
              " 'computers': 0.14285714285714285,\n",
              " 'learn': 0.14285714285714285,\n",
              " 'data': 0.14285714285714285}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse Document Frequency\n",
        "import math\n",
        "\n",
        "def calculate_idf(filtered_tokens, total_documents):\n",
        "  idf_values = {word: math.log(total_documents / 1) for word in filtered_tokens} # log(1) = 0\n",
        "  return idf_values\n",
        "\n",
        "# Given filtered tokens\n",
        "filtered_tokens = ['Machine', 'learning', 'important', 'allows', 'computers', 'learn', 'data']\n",
        "\n",
        "# Total number of documents (since we have only one document, N = 1)\n",
        "total_documents = 1\n",
        "\n",
        "# Calculate IDF\n",
        "idf_results = calculate_idf(filtered_tokens, total_documents)"
      ],
      "metadata": {
        "id": "9oDJcapVQevY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print IDF values\n",
        "print(\"IDF values for the given words:\")\n",
        "\n",
        "for word, idf in idf_results.items():\n",
        "  print(f\"{word}: {idf}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnouoRqKQruI",
        "outputId": "4f9e8304-1683-4c76-b789-f559d566a481"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDF values for the given words:\n",
            "Machine: 0.0\n",
            "learning: 0.0\n",
            "important: 0.0\n",
            "allows: 0.0\n",
            "computers: 0.0\n",
            "learn: 0.0\n",
            "data: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF=> Term Frequency-Inverse Document Frequency\n",
        "def calculate_tf_idf(tf_values, idf_values):\n",
        "  tf_idf_values = {word: tf_values[word] * idf_values[word] for word in tf_values}\n",
        "  return tf_idf_values\n",
        "\n",
        "tf_idf_results = calculate_tf_idf(tf, idf_results)"
      ],
      "metadata": {
        "id": "A5n2v-t2QuGg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print TF-IDF values\n",
        "print(\"TF-IDF values for the given words:\")\n",
        "for word, tf_idf in tf_idf_results.items():\n",
        "  print(f\"{word}: {tf_idf}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0LxopmfQzUA",
        "outputId": "8ee4d85f-1d79-497b-fb6a-f6fc19e53779"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF values for the given words:\n",
            "Machine: 0.0\n",
            "learning: 0.0\n",
            "important: 0.0\n",
            "allows: 0.0\n",
            "computers: 0.0\n",
            "learn: 0.0\n",
            "data: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End of Program"
      ],
      "metadata": {
        "id": "groKrDOJRLVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# Alternate Method for all above steps**\n",
        "- Use TfidfVectorizer() from scikit-learn\n",
        "- Handles preprocessing automatically (removes stop words, normalizes words)"
      ],
      "metadata": {
        "id": "LluFafv7RCQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_filtered = \"Machine learning important allows computers learn data\"\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf_matrix = vectorizer.fit_transform([text_filtered])\n",
        "\n",
        "# Extract feature names (words) from the vectorizer\n",
        "tfidf_feature_names = vectorizer.get_feature_names_out()\n"
      ],
      "metadata": {
        "id": "hqtUoaSYQ2Kw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display TF, IDF, and TF-IDF values\n",
        "print(\"\\nWord\\tTF\\tIDF\\tTF-IDF\")\n",
        "for i, word in enumerate(tfidf_feature_names):\n",
        "  idf = vectorizer._tfidf.idf_[i] # Access IDF value from the vectorizer\n",
        "  tfidf_value = tfidf_matrix[0, i]\n",
        "  print(f\"{word}\\t{tf.get(word, 0):.4f}\\t{idf:.4f}\\t{tfidf_value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7CIPsMZRaQn",
        "outputId": "6e6e4f76-1ae2-416c-fb3c-8d4d48e21a14"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word\tTF\tIDF\tTF-IDF\n",
            "allows\t0.1429\t1.0000\t0.3780\n",
            "computers\t0.1429\t1.0000\t0.3780\n",
            "data\t0.1429\t1.0000\t0.3780\n",
            "important\t0.1429\t1.0000\t0.3780\n",
            "learn\t0.1429\t1.0000\t0.3780\n",
            "learning\t0.1429\t1.0000\t0.3780\n",
            "machine\t0.0000\t1.0000\t0.3780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TfidfVectorizer() gives different results than manual calculations because of\n",
        "- Default settings like sublinear TF scaling, smoothing, stop-word removal, L2 Normalization etc"
      ],
      "metadata": {
        "id": "iQ2gSBPaSDES"
      }
    }
  ]
}